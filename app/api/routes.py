from typing import Optional, List
from fastapi import APIRouter, HTTPException, Query
from fastapi.responses import RedirectResponse
import json
import psycopg2
from psycopg2.extras import execute_values
from psycopg2 import Error
from app.schemas.schemas import (
    FileCreate,
    Project,
    Dataset,
    DatasetMetadata,
    DatasetWithMetadata,
    Patient,
    PatientWithSampleCount,
    PatientMetadata,
    PatientWithMetadata,
    SampleMetadata,
    Sample,
    SampleWithoutPatient,
    FileResponse,
    PatientWithSamples,
    FileMetadataCreate,
    MetadataUpdate,
)
import logging

# Replace with your actual connection details
DB_NAME = "readmedatabase"
DB_USER = "postgres"
DB_PASSWORD = "password"
DB_HOST = "localhost"
DB_PORT = "5432"

logger = logging.getLogger('uvicorn.error')
logger.setLevel(logging.DEBUG)

router = APIRouter()



def get_connection():
    """
    Helper function to get a new connection to your PostgreSQL database.
    """
    return psycopg2.connect(
        dbname=DB_NAME,
        user=DB_USER,
        password=DB_PASSWORD,
        host=DB_HOST,
        port=DB_PORT
    )


@router.post("/add_files/")
async def add_files(files: List[FileCreate]):
    try:
        conn = get_connection()
        cursor = conn.cursor()

        file_ids = []
        for file in files:
            cursor.execute(
                """
                INSERT INTO files (dataset_id, path, file_type)
                VALUES (%s, %s, %s)
                RETURNING id
                """,
                (file.dataset_id, file.path, file.file_type)
            )
            file_id = cursor.fetchone()[0]
            file_ids.append(file_id)

            if file.metadata:
                for metadata in file.metadata:
                    cursor.execute(
                        """
                        INSERT INTO files_metadata (file_id, metadata_key, metadata_value)
                        VALUES (%s, %s, %s)
                        """,
                        (file_id, metadata.metadata_key, metadata.metadata_value)
                    )

        conn.commit()
        conn.close()
        return {"status": "success", "message": "Files and metadata added successfully"}

    except Error as e:
        raise HTTPException(status_code=500, detail=f"Database error: {e}")


@router.get("/")
async def root():
    
    return RedirectResponse(url="/projects")


@router.get("/patients_metadata/{patient_id}", response_model=List[PatientWithSamples])
async def get_patients_metadata(project_id: int, patient_id: int):
    """
    Fetch patients (and their samples + metadata) for a given project_id.
    If patient_id == 0, fetch all patients; otherwise, fetch the specified patient.
    """
    try:
        conn = get_connection()
        cursor = conn.cursor()

        if patient_id != 0:
            cursor.execute(
                """
                SELECT p.id, p.project_id, p.ext_patient_id, p.ext_patient_url, p.public_patient_id,
                       pm.id, pm.key, pm.value
                FROM patients p
                LEFT JOIN patients_metadata pm ON p.id = pm.patient_id
                WHERE p.project_id = %s AND p.id = %s
                ORDER BY p.id
                """,
                (project_id, patient_id)
            )
        else:
            cursor.execute(
                """
                SELECT p.id, p.project_id, p.ext_patient_id, p.ext_patient_url, p.public_patient_id,
                       pm.id, pm.key, pm.value
                FROM patients p
                LEFT JOIN patients_metadata pm ON p.id = pm.patient_id
                WHERE p.project_id = %s
                ORDER BY p.id
                """,
                (project_id,)
            )

        rows = cursor.fetchall()

        patients = []
        current_patient = None
        for row in rows:
            # row = [p.id, p.project_id, p.ext_patient_id, p.ext_patient_url, p.public_patient_id,
            #        pm.id, pm.key, pm.value]
            if not current_patient or current_patient['id'] != row[0]:
                if current_patient:
                    patients.append(current_patient)
                current_patient = {
                    'id': row[0],
                    'project_id': row[1],
                    'ext_patient_id': row[2],
                    'ext_patient_url': row[3],
                    'public_patient_id': row[4],
                    'samples': [],
                    'metadata': []
                }

            if row[5]:  # pm.id is not None
                current_patient['metadata'].append({
                    'id': row[5],
                    'patient_id': row[0],
                    'key': row[6],
                    'value': row[7]
                })

        if current_patient:
            patients.append(current_patient)

        # Now fetch samples for each patient
        for patient in patients:
            cursor.execute(
                """
                SELECT s.id, s.patient_id, s.ext_sample_id, s.ext_sample_url,
                       sm.id, sm.key, sm.value
                FROM samples s
                LEFT JOIN samples_metadata sm ON s.id = sm.sample_id
                WHERE s.patient_id = %s
                ORDER BY s.id
                """,
                (patient['id'],)
            )

            sample_rows = cursor.fetchall()
            current_sample = None
            for sample_row in sample_rows:
                # sample_row = [s.id, s.patient_id, s.ext_sample_id, s.ext_sample_url,
                #               sm.id, sm.key, sm.value]
                if not current_sample or current_sample['id'] != sample_row[0]:
                    if current_sample:
                        patient['samples'].append(current_sample)
                    current_sample = {
                        'id': sample_row[0],
                        'patient_id': sample_row[1],
                        'ext_sample_id': sample_row[2],
                        'ext_sample_url': sample_row[3],
                        'metadata': []
                    }
                if sample_row[4]:  # sm.id is not None
                    current_sample['metadata'].append({
                        'id': sample_row[4],
                        'sample_id': sample_row[0],
                        'key': sample_row[5],
                        'value': sample_row[6]
                    })

            if current_sample:
                patient['samples'].append(current_sample)

        conn.close()
        return patients

    except Error as e:
        raise HTTPException(status_code=500, detail=f"Database error: {e}")


@router.get("/samples/{sample_id}", response_model=List[Sample])
async def get_samples_per_patient(sample_id: int, project_id: int):
    """
    Fetch samples (and their metadata) for a given project_id, optionally filtering by sample_id.
    """
    try:
        conn = get_connection()
        cursor = conn.cursor()

        if sample_id != 0:
            cursor.execute(
                """
                SELECT s.id AS sample_id, s.patient_id, s.ext_sample_id, s.ext_sample_url,
                       sm.id AS metadata_id, sm.key, sm.value,
                       p.id AS patient_id, p.project_id, p.ext_patient_id, p.ext_patient_url, p.public_patient_id
                FROM samples s
                LEFT JOIN samples_metadata sm ON s.id = sm.sample_id
                LEFT JOIN patients p ON s.patient_id = p.id
                WHERE p.project_id = %s AND s.id = %s
                ORDER BY s.id, sm.id
                """,
                (project_id, sample_id)
            )
        else:
            cursor.execute(
                """
                SELECT s.id AS sample_id, s.patient_id, s.ext_sample_id, s.ext_sample_url,
                       sm.id AS metadata_id, sm.key, sm.value,
                       p.id AS patient_id, p.project_id, p.ext_patient_id, p.ext_patient_url, p.public_patient_id
                FROM samples s
                LEFT JOIN samples_metadata sm ON s.id = sm.sample_id
                LEFT JOIN patients p ON s.patient_id = p.id
                WHERE p.project_id = %s
                ORDER BY s.id, sm.id
                """,
                (project_id,)
            )

        rows = cursor.fetchall()
        conn.close()

        samples = []
        current_sample = None

        for row in rows:
            # row = [sample_id, patient_id, ext_sample_id, ext_sample_url,
            #        metadata_id, key, value, pat_id, project_id, ext_pat_id, ext_pat_url, public_pat_id]
            if not current_sample or current_sample['id'] != row[0]:
                if current_sample:
                    samples.append(current_sample)
                current_sample = {
                    'id': row[0],
                    'patient_id': row[1],
                    'ext_sample_id': row[2],
                    'ext_sample_url': row[3],
                    'metadata': [],
                    'patient': {
                        'id': row[7],
                        'project_id': row[8],
                        'ext_patient_id': row[9],
                        'ext_patient_url': row[10],
                        'public_patient_id': row[11]
                    }
                }

            if row[4]:  # metadata_id is not None
                current_sample['metadata'].append({
                    'id': row[4],
                    'sample_id': row[0],
                    'key': row[5],
                    'value': row[6]
                })

        if current_sample:
            samples.append(current_sample)

        return samples

    except Error as e:
        raise HTTPException(status_code=500, detail=f"Database error: {e}")


@router.get("/patients/", response_model=List[PatientWithSampleCount])
async def get_patients(
        project_id: Optional[int] = Query(None, description="Filter by project ID")
):
    """
    Fetch all patients (optionally filtered by project_id) with a count of how many samples they have.
    """
    try:
        conn = get_connection()
        cursor = conn.cursor()

        query = """
            SELECT p.id, p.project_id, p.ext_patient_id, p.ext_patient_url,
                   p.public_patient_id, COUNT(s.id) AS sample_count
            FROM patients p
            LEFT JOIN samples s ON p.id = s.patient_id
        """
        params = []

        if project_id is not None:
            query += " WHERE p.project_id = %s"
            params.append(project_id)

        query += " GROUP BY p.id ORDER BY p.id"

        cursor.execute(query, params)
        rows = cursor.fetchall()
        conn.close()

        patients = []
        for row in rows:
            patients.append({
                'id': row[0],
                'project_id': row[1],
                'ext_patient_id': row[2],
                'ext_patient_url': row[3],
                'public_patient_id': row[4],
                'sample_count': row[5]
            })

        return patients

    except Error as e:
        raise HTTPException(status_code=500, detail=f"Database error: {e}")


@router.get("/projects/", response_model=List[Project])
async def get_projects():
    try:
        conn = get_connection()
        cursor = conn.cursor()

        cursor.execute("SELECT id, name, status FROM projects")
        rows = cursor.fetchall()
        conn.close()

        return [Project(id=row[0], name=row[1], status=row[2]) for row in rows]

    except Error as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Database error: {e}")


@router.get("/datasets/", response_model=List[Dataset])
async def get_datasets(
        project_id: Optional[int] = Query(None, description="Filter by project ID"),
        dataset_id: Optional[int] = Query(None, description="Filter by dataset ID")
):
    """
    Fetch datasets, optionally filtered by project_id and/or dataset_id.
    """
    try:
        conn = get_connection()
        cursor = conn.cursor()

        query = "SELECT id, project_id, name FROM datasets WHERE 1=1"
        params = []

        if project_id is not None:
            query += " AND project_id = %s"
            params.append(project_id)

        if dataset_id is not None:
            query += " AND id = %s"
            params.append(dataset_id)

        cursor.execute(query, params)
        rows = cursor.fetchall()
        conn.close()

        return [Dataset(id=row[0], project_id=row[1], name=row[2]) for row in rows]

    except Error as e:
        raise HTTPException(status_code=500, detail=f"Database error: {e}")


@router.get("/datasets_with_metadata/{dataset_id}", response_model=DatasetWithMetadata)
async def get_dataset_with_metadata(dataset_id: int, project_id: int):
    """
    Fetch dataset details (and its metadata) for the given dataset_id + project_id.
    """
    try:
        conn = get_connection()
        cursor = conn.cursor()

        # Fetch dataset
        cursor.execute(
            """
            SELECT id, project_id, name
            FROM datasets
            WHERE id = %s AND project_id = %s
            """,
            (dataset_id, project_id)
        )
        dataset_row = cursor.fetchone()
        if not dataset_row:
            raise HTTPException(status_code=404, detail="Dataset not found")

        # Fetch dataset metadata
        cursor.execute(
            """
            SELECT id, dataset_id, key, value
            FROM datasets_metadata
            WHERE dataset_id = %s
            """,
            (dataset_id,)
        )
        metadata_rows = cursor.fetchall()
        conn.close()

        dataset = {
            "id": dataset_row[0],
            "project_id": dataset_row[1],
            "name": dataset_row[2],
            "metadata": [
                {"id": row[0], "dataset_id": row[1], "key": row[2], "value": row[3]}
                for row in metadata_rows
            ],
        }
        return dataset

    except Error as e:
        raise HTTPException(status_code=500, detail=f"Database error: {e}")


@router.get("/files_with_metadata/{dataset_id}", response_model=List[FileResponse])
async def get_files_with_metadata(dataset_id: int):
    """
    Fetch files within a dataset, along with any related sample metadata (based on sample_id stored as metadata).
    """
    try:
        conn = get_connection()
        cursor = conn.cursor()

        # Query files and the sample relationship from files_metadata
        query = """
            SELECT f.id, f.path, fm.metadata_value AS sample_id, s.ext_sample_id
            FROM files f
            LEFT JOIN files_metadata fm ON f.id = fm.file_id
            LEFT JOIN samples s ON fm.metadata_value = CAST(s.id AS TEXT)
            WHERE f.dataset_id = %s AND fm.metadata_key = 'sample_id'
        """
        cursor.execute(query, (dataset_id,))
        files = cursor.fetchall()

        response = []

        for (file_id, path, sample_id, ext_sample_id) in files:
            # Fetch sample metadata
            cursor.execute(
                """
                SELECT id, sample_id, key, value
                FROM samples_metadata
                WHERE sample_id = %s
                """,
                (sample_id,)
            )
            sample_metadata_rows = cursor.fetchall()
            sample_metadata_list = []
            for row in sample_metadata_rows:
                sample_metadata_list.append({
                    'id': row[0],
                    'sample_id': row[1],
                    'key': row[2],
                    'value': row[3]
                })

            response.append(FileResponse(
                id=file_id,
                path=path,
                sample_id=int(sample_id) if sample_id is not None else None,
                ext_sample_id=ext_sample_id,
                sample_metadata=sample_metadata_list
            ))

        conn.close()
        return response

    except Error as e:
        raise HTTPException(status_code=500, detail=f"Database error: {e}")


@router.put("/datasets_metadata/size_update", response_model=MetadataUpdate)
def update_metadata(update: MetadataUpdate):
    """
    Update specific metadata fields in the datasets_metadata table:
      - file_extension_size_of_all_files
      - last_size_update
    for the given dataset_id.
    """
    try:
        conn = get_connection()
        cursor = conn.cursor()

        # 1) Update or insert 'file_extension_size_of_all_files'
        if update.file_size:
            cursor.execute(
                """
                SELECT id, value
                FROM datasets_metadata
                WHERE key = 'file_extension_size_of_all_files'
                  AND dataset_id = %s
                """,
                (update.dataset_id,)
            )
            record = cursor.fetchone()
            if record:
                record_id, _ = record
                cursor.execute(
                    """
                    UPDATE datasets_metadata
                    SET value = %s
                    WHERE id = %s
                    """,
                    (update.file_size, record_id)
                )
            else:
                cursor.execute(
                    """
                    INSERT INTO datasets_metadata (dataset_id, key, value)
                    VALUES (%s, 'file_extension_size_of_all_files', %s)
                    """,
                    (update.dataset_id, update.file_size)
                )

        # 2) Update or insert 'last_size_update'
        if update.last_size_update:
            cursor.execute(
                """
                SELECT id, value
                FROM datasets_metadata
                WHERE key = 'last_size_update'
                  AND dataset_id = %s
                """,
                (update.dataset_id,)
            )
            record = cursor.fetchone()
            if record:
                record_id, _ = record
                cursor.execute(
                    """
                    UPDATE datasets_metadata
                    SET value = %s
                    WHERE id = %s
                    """,
                    (update.last_size_update, record_id)
                )
            else:
                cursor.execute(
                    """
                    INSERT INTO datasets_metadata (dataset_id, key, value)
                    VALUES (%s, 'last_size_update', %s)
                    """,
                    (update.dataset_id, update.last_size_update)
                )

        conn.commit()
        conn.close()
        return update

    except Error as e:
        raise HTTPException(status_code=500, detail=f"Database error: {e}")

@router.post("/ingest/simple_raw_file_test")
async def simple_raw_file_test():
    logging.debug("--- Starting simple raw file test endpoint ---")

    # will be provided as an input
    dataset_id_to_use = 1 
    json_file_path = "output.json"
    
    conn = None
    try:
        logging.debug(f"Attempting to read file at: {json_file_path}")
        with open(json_file_path, 'r') as f:
            ingestion_data = json.load(f)
        logging.debug("File read and parsed successfully.")

        raw_files = ingestion_data['data']['files']['raw']
        logging.debug(f"Found {len(raw_files)} raw files to process.")
        
        logging.debug("Connecting to the database...")
        conn = get_connection()
        cursor = conn.cursor()
        logging.debug("Database connection successful.")

        for i, file_detail in enumerate(raw_files):
            logging.debug(f"Processing file {i+1}: {file_detail['file_name']}")
            
            cursor.execute(
                """
                INSERT INTO files (dataset_id, path, file_type)
                VALUES (%s, %s, %s)
                RETURNING id
                """,
                (dataset_id_to_use, file_detail['directory'], 'raw')
            )
            file_id = cursor.fetchone()[0]
            logging.debug(f"  -> Inserted into 'files' table with new ID: {file_id}")

            metadata_to_insert = [
                (file_id, 'file_name', file_detail['file_name']),
                (file_id, 'file_size', str(file_detail['file_size'])),
                (file_id, 'organization', file_detail['organization']),
                (file_id, 'patient_id', file_detail['patient_id']),
                (file_id, 'sample_id', file_detail['sample_id']),
            ]
            
            execute_values(
                cursor,
                "INSERT INTO files_metadata (file_id, metadata_key, metadata_value) VALUES %s",
                metadata_to_insert
            )
            logging.debug(f"  -> Inserted {len(metadata_to_insert)} metadata records.")

        logging.debug("All files processed. Committing transaction...")
        conn.commit()
        logging.debug("Transaction committed successfully.")
        return {"status": "success", "message": f"{len(raw_files)} raw files have been inserted for dataset ID {dataset_id_to_use}."}

    except FileNotFoundError:
        logging.debug(f"ERROR: The file was not found at '{json_file_path}'.")
        raise HTTPException(status_code=500, detail=f"Error: The file was not found at '{json_file_path}'. Make sure it's in the project root directory.")
    except KeyError as e:
        logging.debug(f"ERROR: A key was not found in the JSON file: {e}")
        raise HTTPException(status_code=500, detail=f"Error: The JSON file is missing an expected key: {e}")
    except Error as e:
        logging.debug(f"DATABASE ERROR: {e}")
        if conn:
            conn.rollback() 
        raise HTTPException(status_code=500, detail=f"Database transaction failed: {e}")
    finally:
        if conn:
            conn.close()
            logging.debug("Database connection closed.")


@router.get('/testing')
async def testing():
    print("testing api call")
    logging.debug("testing call")

    return {"message": "success"}